# Micrograd-PLUS

A **minimal autograd engine and neural network library in Python**, implemented from scratch and inspired by Andrej Karpathyâ€™s "micrograd" lecture.

While this project follows the core concepts presented in the lecture, several features have been independently added to extend functionality and usability, including:

- Multiple **activation functions**: ReLU, Sigmoid, and Tanh  
- Modular **MLP, Layer, and Neuron classes**  
- **Loss functions**: Mean Squared Error (MSE) and Cross-Entropy  
- **Optimizer**: Stochastic Gradient Descent (SGD)  
- Examples for **simple regression and classification**

This project was built as a **learning-focused exercise**, demonstrating a deep understanding of backpropagation, computational graphs, and neural network training while providing a **reusable framework** for small experiments and custom extensions.
